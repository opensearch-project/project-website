---
layout: post
title: "Ingest data from Kafka into OpenSearch using Kafka Connect - Part 1 Local Development"
authors: 
  - jhkim
date: 2023-09-28
meta_keywords: OpenSearch, Apache Kafka, Kafka Connect, Amazon MSK, Amazon MSK Connect, Docker
meta_description: Read on to learn about how to ingest data from Apache Kafka into OpenSaerch using Kafka Connect.
twittercard:
  description: Learn about how to ingest data from Apache Kafka into OpenSaerch using Kafka Connect.
category:
  - community
excerpt: 
  We can use Kafka Connect to ingesting data from Apache Kafka into OpenSearch. In this post, we will discuss how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker while the pipeline will be deployed on AWS in the next post. Fake impressions and clicks data will be pushed into Kafka topics using a Kafka source connector and those records will be ingested into OpenSearch indexes using a sink connector for near-real time analytics.
feature_image: /assets/media/blog-images/2023-09-28-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-1-Local-Development/featured.png
---

[OpenSearch](https://opensearch.org/) is a popular search and analytics engine and its use cases cover log analytics, real-time application monitoring, and clickstream analysis. OpenSearch can be deployed on its own or via [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/). [Apache Kafka](https://kafka.apache.org/) is a distributed event store and stream-processing platform, and it aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. On AWS, Apache Kafka can be deployed via [Amazon Managed Streaming for Apache Kafka (MSK)](https://aws.amazon.com/msk/).

When it comes to ingesting data from Apache Kafka into OpenSearch, OpenSearch has a tool called [Data Prepper](https://opensearch.org/docs/latest/data-prepper/index/) and Amazon OpenSearch Service has a feature called [Amazon OpenSearch Ingestion](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ingestion.html). Alternatively we can use [Kafka Connect](https://kafka.apache.org/documentation/#connect), which is a tool for scalably and reliably streaming data between Apache Kafka and other systems. On AWS, we can run fully managed Kafka workload using [Amazon MSK Connect](https://aws.amazon.com/msk/features/msk-connect/).

In this post, we will discuss how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker while the pipeline will be deployed on AWS in the next post. Fake impressions and clicks data will be pushed into Kafka topics using a Kafka source connector and those records will be ingested into OpenSearch indexes using a sink connector for near-real time analytics.

## Architecture

Fake impressions and clicks events are generated by the [Amazon MSK Data Generator](https://github.com/awslabs/amazon-msk-data-generator), and they are pushed into the corresponding Kafka topics. The topic records are ingested into OpenSearch indexes with the same names for near real-time analysis using the [Aiven's OpenSearch Connector for Apache Kafka](https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka). The infrastructure is created using [Docker](https://www.docker.com/) and the source can be found in the [**GitHub repository**](https://github.com/jaehyeon-kim/general-demos/tree/master/opensearch-kafka-connect/local) of this post.

![architecture]({{ site.baseurl }}/assets/media/blog-images/2023-09-28-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-1-Local-Development/featured.png){: .img-fluid }

## OpenSearch Cluster

We can create OpenSearch and OpenSearch Dashboards using [Docker Compose](https://docs.docker.com/compose/) as illustrated in the OpenSearch [Quickstart documentation](https://opensearch.org/docs/latest/quickstart/). The OpenSearch stack consists of two cluster nodes and a single dashboard service. Note that you should disable memory paging and swapping performance on the host before creating those - see the documentation for details.

```yaml
# docker-compose.yml
version: "3.5"

services:

  ...

  opensearch-node1:
    image: opensearchproject/opensearch:2.9.0
    container_name: opensearch-node1
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-node1-data:/usr/share/opensearch/data
    ports:
      - 9200:9200
      - 9600:9600
    networks:
      - service-net
  opensearch-node2:
    image: opensearchproject/opensearch:2.9.0
    container_name: opensearch-node2
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node2
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-node1-data:/usr/share/opensearch/data
    networks:
      - service-net
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.9.0
    container_name: opensearch-dashboards
    ports:
      - 5601:5601
    expose:
      - "5601"
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch-node1:9200","https://opensearch-node2:9200"]'
    networks:
      - service-net

networks:
  service-net:
    name: service-net

volumes:

  ...

  opensearch-node1-data:
    driver: local
    name: opensearch-node1-data
  opensearch-node2-data:
    driver: local
    name: opensearch-node2-data
```


## Kafka Cluster

We will create a Kafka cluster with a single broker and Zookeeper node using the [Bitnami container images](https://github.com/bitnami/containers/tree/main). *Kafka 2.8.1* is used as it is the [recommended Kafka version by Amazon MSK](https://docs.aws.amazon.com/msk/latest/developerguide/supported-kafka-versions.html#2.8.1). The cluster communicates on port 9092 internally and the bootstrap server of the broker becomes *kafka-0:9092* for those that run in the same network.

```yaml
# docker-compose.yml
version: "3.5"

services:
  zookeeper:
    image: bitnami/zookeeper:3.5
    container_name: zookeeper
    ports:
      - "2181"
    networks:
      - service-net
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper-data:/bitnami/zookeeper
  kafka-0:
    image: bitnami/kafka:2.8.1
    container_name: kafka-0
    expose:
      - 9092
    ports:
      - "29092:29092"
    networks:
      - service-net
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_BROKER_ID=0
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka-0:9092,EXTERNAL://localhost:29092
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_NUM_PARTITIONS=2
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1
    volumes:
      - kafka-0-data:/bitnami/kafka
    depends_on:
      - zookeeper

   ...

volumes:
  zookeeper-data:
    driver: local
    name: zookeeper-data
  kafka-0-data:
    driver: local
    name: kafka-0-data

  ...
```

## Kafka Connect

We can use the same Docker image as [*Kafka Connect*](https://kafka.apache.org/documentation/#connect) is a part of the Kafka distribution. The Kafka connect server runs in the [distributed mode](https://docs.confluent.io/platform/current/connect/index.html#distributed-workers) so that multiple connectors can be deployed to the same server. Note the Kafka bootstrap server and plugin path configuration values in the connect configuration file ([connect-distributed.properties](https://github.com/jaehyeon-kim/general-demos/blob/master/opensearch-kafka-connect/local/configs/connect-distributed.properties)) listed below. As mentioned earlier, the bootstrap server can be accessed on *kafka-0:9092* as it is deployed in the same network. Also, we will locate connector source files in subdirectories of the plugin path.

- *bootstrap.servers=kafka-0:9092*
- *plugin.path=/opt/connectors*


```yaml
# docker-compose.yml
version: "3.5"

services:

  ...

  kafka-connect:
    image: bitnami/kafka:2.8.1
    container_name: connect
    command: >
      /opt/bitnami/kafka/bin/connect-distributed.sh
      /opt/bitnami/kafka/config/connect-distributed.properties
    ports:
      - "8083:8083"
    networks:
      - service-net
    volumes:
      - "./configs/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties"
      - "./connectors/opensearch-connector:/opt/connectors/opensearch-connector"
      - "./connectors/msk-datagen:/opt/connectors/msk-datagen"
    depends_on:
      - zookeeper
      - kafka-0

  ...

```

### Download Connectors

The connector sources need to be downloaded into the `./connectors` path so that they can be volume-mapped to the container's plugin path (`/opt/connectors`). The MSK Data Generator is a single Jar file, and it can be kept as it is. On the other hand, the Aiven OpenSearch sink connector is an archive file, and it should be decompressed. Note the zip file will be used to create a custom plugin of MSK Connect in the next post. The following script downloads the connector sources into the host path.

```bash
# download.sh
#!/usr/bin/env bash
shopt -s extglob

SCRIPT_DIR="$(cd $(dirname "$0"); pwd)"

SRC_PATH=${SCRIPT_DIR}/connectors
rm -rf ${SRC_PATH} && mkdir ${SRC_PATH}

## Avien opensearch sink connector
echo "downloading opensearch sink connector..."
DOWNLOAD_URL=https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka/releases/download/v3.1.0/opensearch-connector-for-apache-kafka-3.1.0.zip

curl -L -o ${SRC_PATH}/tmp.zip ${DOWNLOAD_URL} \
  && unzip -qq ${SRC_PATH}/tmp.zip -d ${SRC_PATH} \
  && rm -rf $SRC_PATH/!(opensearch-connector-for-apache-kafka-3.1.0) \
  && mv $SRC_PATH/opensearch-connector-for-apache-kafka-3.1.0 $SRC_PATH/opensearch-connector \
  && cd $SRC_PATH/opensearch-connector \
  && zip ../opensearch-connector.zip *

## MSK Data Generator Souce Connector
echo "downloading msk data generator..."
DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar

mkdir ${SRC_PATH}/msk-datagen \
  && curl -L -o ${SRC_PATH}/msk-datagen/msk-data-generator.jar ${DOWNLOAD_URL}
```

Below shows the folder structure after the connectors are downloaded successfully.

```bash
$ tree connectors/
connectors/
├── msk-datagen
│   └── msk-data-generator.jar
├── opensearch-connector

...

│   ├── opensearch-2.6.0.jar
│   ├── opensearch-cli-2.6.0.jar
│   ├── opensearch-common-2.6.0.jar
│   ├── opensearch-connector-for-apache-kafka-3.1.0.jar
│   ├── opensearch-core-2.6.0.jar

...

└── opensearch-connector.zip

2 directories, 56 files
```

## Kafka Management App

A Kafka management app can be a good companion for development as it helps monitor and manage resources on an easy-to-use user interface. We'll use [Kpow Community Edition](https://docs.kpow.io/ce/) in this post, and we can link a single Kafka cluster, Kafka connect server and schema registry. Note that the community edition is valid for 12 months and the license can be requested in this [page](https://kpow.io/get-started/#individual). Once requested, the license details will be emailed, and they can be added as an environment file (*env_file*).

```yaml
# docker-compose.yml
version: "3.5"

services:

  ...

  kpow:
    image: factorhouse/kpow-ce:91.5.1
    container_name: kpow
    ports:
      - "3000:3000"
    networks:
      - service-net
    environment:
      BOOTSTRAP: kafka-0:9092
      CONNECT_REST_URL: http://kafka-connect:8083
    env_file: # https://kpow.io/get-started/#individual
      - ./kpow.env
  
  ...

```

## Data Ingestion to Kafka Topic

### Create Index Mappings

The topic messages include a timestamp field (*created_at*), but its type is not identified correctly via dynamic mapping. Instead, indexes are created explicitly as shown below.

```bash
# configs/create-index-mappings.sh
#!/usr/bin/env bash
echo "Create impressions index and field mapping"
curl -X PUT "https://localhost:9200/impressions" -ku admin:admin -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "bid_id": {
        "type": "text"
      },
      "created_at": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss"
      },
      "campaign_id": {
        "type": "text"
      },
      "creative_details": {
        "type": "keyword"
      },
      "country_code": {
        "type": "keyword"
      }
    }
  }
}'

echo
echo "Create clicks index and field mapping"
curl -X PUT "https://localhost:9200/clicks" -ku admin:admin -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "correlation_id": {
        "type": "text"
      },
      "created_at": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss"
      },
      "tracker": {
        "type": "text"
      }
    }
  }
}'

echo
```

Once created, we can check them in the OpenSearch Dashboards on *localhost:5601*.

![index_creation]({{ site.baseurl }}/assets/media/blog-images/2023-09-28-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-1-Local-Development/index-creation.png){: .img-fluid }

### Source Connector Creation

We can create the source connector programmatically using the Connect REST API. The REST endpoint requires a JSON payload that includes the connector name and configurations.

```bash
$ curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" \
  http://localhost:8083/connectors/ -d @configs/source.json
```

Below shows the source connector configuration file.

```json
// configs/source.json
{
  "name": "ad-tech-source",
  "config": {
    "connector.class": "com.amazonaws.mskdatagen.GeneratorSourceConnector",
    "tasks.max": "2",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "key.converter.schemas.enable": false,
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": false,

    "genkp.impressions.with": "#{Code.isbn10}",
    "genv.impressions.bid_id.with": "#{Code.isbn10}",
    "genv.impressions.campaign_id.with": "#{Code.isbn10}",
    "genv.impressions.creative_details.with": "#{Color.name}",
    "genv.impressions.country_code.with": "#{Address.countryCode}",

    "genkp.clicks.with": "#{Code.isbn10}",
    "genv.clicks.correlation_id.sometimes.matching": "impressions.value.bid_id",
    "genv.clicks.correlation_id.sometimes.with": "NA",
    "genv.clicks.tracker.with": "#{Lorem.characters '15'}",

    "global.throttle.ms": "500",
    "global.history.records.max": "1000"
  }
}
```

The first six attributes are in relation to general configurations. The connector class (*connector.class*) is required for any connector and I set it for the MSK Data Generator. Also, two tasks are allocated to it (*tasks.max*). The message key is set to be converted into string (*key.converter*) while the value to json (*value.converter*). The former is because the keys are configured to have string primitive values (*genkp*) by the source connector. Finally, schemas are not enabled for both the key and value.

The remaining attributes are for the MSK Data Generator. Two topics named *impressions* and *clicks* will be created, and the messages attributes are generated by the [Java faker library](https://github.com/DiUS/java-faker). Interestingly the bid ID of the impression message and the correlation ID of the click message share the same value *sometimes*. This is because only a fraction of impressions results in clicks in practice.

Once created successfully, we can check the connector status as following.

```bash
$ curl http://localhost:8083/connectors/ad-tech-source/status | json_pp
{
   "connector" : {
      "state" : "RUNNING",
      "worker_id" : "172.19.0.8:8083"
   },
   "name" : "ad-tech-source",
   "tasks" : [
      {
         "id" : 0,
         "state" : "RUNNING",
         "worker_id" : "172.19.0.8:8083"
      },
      {
         "id" : 1,
         "state" : "RUNNING",
         "worker_id" : "172.19.0.8:8083"
      }
   ],
   "type" : "source"
}
```

### Source Data

We can check messages are ingested into the impressions and clicks topics in Kpow on *localhost:3000*.

![kafka_topics]({{ site.baseurl }}/assets/media/blog-images/2023-09-28-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-1-Local-Development/kafka-topics.png){: .img-fluid }

As mentioned earlier, only a fraction of correlation IDs of the click messages has actual values, and we can check that by inspecting the messages.

![click_messages]({{ site.baseurl }}/assets/media/blog-images/2023-09-28-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-1-Local-Development/click-messages.png){: .img-fluid }

## Data Ingestion to OpenSearch

### Sink Connector Creation

Similar to the source connector, we can create the sink connector using the REST API.

```bash
$ curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" \
  http://localhost:8083/connectors/ -d @configs/sink.json
```

Below shows the sink connector configuration file.

```json
// configs/sink.json
{
  "name": "ad-tech-sink",
  "config": {
    "connector.class": "io.aiven.kafka.connect.opensearch.OpensearchSinkConnector",
    "tasks.max": "2",
    "topics": "impressions,clicks",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "key.converter.schemas.enable": false,
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": false,

    "connection.url": "https://opensearch-node1:9200,https://opensearch-node2:9200",
    "connection.username": "admin",
    "connection.password": "admin",
    "schema.ignore": true,
    "key.ignore": true,
    "type.name": "_doc",
    "behavior.on.malformed.documents": "fail",
    "behavior.on.null.values": "ignore",
    "behavior.on.version.conflict": "ignore",

    "errors.deadletterqueue.topic.name": "ad-tech-dl",
    "errors.tolerance": "all",
    "errors.deadletterqueue.context.headers.enable": true,
    "errors.deadletterqueue.topic.replication.factor": 1,

    "transforms": "insertTS,formatTS",
    "transforms.insertTS.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.insertTS.timestamp.field": "created_at",
    "transforms.formatTS.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
    "transforms.formatTS.format": "yyyy-MM-dd HH:mm:ss",
    "transforms.formatTS.field": "created_at",
    "transforms.formatTS.target.type": "string"
  }
}
```

The connector is configured to write messages from the *impressions* and *clicks* topics into the OpenSearch indexes created earlier. It uses the same key and value converters to the source connector, and schemas are not enabled for both the key and value.

The cluster request URLs are added to the connection URL attribute (*connection.url*), and the default username and password are specified accordingly. These values are necessary to make HTTP requests to the cluster as shown earlier when we created indexes with explicit schema mapping. Also, as the topics are *append-only* logs, we can set the document ID to be *[topic-name].[partition].[offset]* by setting *key.ignore* to *true*. See the [connector configuration document](https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka/blob/main/docs/opensearch-sink-connector-config-options.rst) for more details. 

Having an event timestamp attribute can be useful for performing temporal analysis. As I don't find a comprehensive way to set it up in the source connector, a new field called *created_at* is added using [single message transforms (SMTs)](https://kafka.apache.org/documentation.html#connect_transforms). Specifically I added two transforms - *insertTS* and *formatTS*. As the name suggests, the former inserts the system timestamp value while it is formatted into *yyyy-MM-dd HH:mm:ss* by the latter.

Once created successfully, we can check the connector status as following.

```bash
$ curl http://localhost:8083/connectors/ad-tech-sink/status | json_pp
{
   "connector" : {
      "state" : "RUNNING",
      "worker_id" : "172.20.0.8:8083"
   },
   "name" : "ad-tech-sink",
   "tasks" : [
      {
         "id" : 0,
         "state" : "RUNNING",
         "worker_id" : "172.20.0.8:8083"
      },
      {
         "id" : 1,
         "state" : "RUNNING",
         "worker_id" : "172.20.0.8:8083"
      }
   ],
   "type" : "sink"
}
```

### OpenSearch Destination

In OpenSearch Dashboards, we can search clicks that are associated with impressions. As expected, only a small portion of clicks are searched. 

![search_result]({{ site.baseurl }}/assets/media/blog-images/2023-09-28-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-1-Local-Development/result-query.png){: .img-fluid }

Moreover, we can join correlated impressions and clicks quickly using the [Query Workbench](https://opensearch.org/docs/latest/search-plugins/sql/sql/index/). Below shows a simple SQL query that joins impressions and associating clicks that are created after a certain time point.

![query_join]({{ site.baseurl }}/assets/media/blog-images/2023-09-28-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-1-Local-Development/result-join.png){: .img-fluid }

## Summary

Kafka Connect can be effective to ingesting data from Apache Kafka into OpenSearch. In this post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. Fake impressions and clicks data was pushed into Kafka topics using a Kafka source connector and those records will be ingested into OpenSearch indexes using a sink connector for near-real time analytics. In this next post, the pipeline will be deployed on AWS using Amazon OpenSearch Service, Amazon MSK and Amazon MSK Connect.