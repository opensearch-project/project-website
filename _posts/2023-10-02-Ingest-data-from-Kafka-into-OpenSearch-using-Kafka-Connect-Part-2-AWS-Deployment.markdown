---
layout: post
title: "Ingest data from Kafka into OpenSearch using Kafka Connect - Part 2 AWS Deployment"
authors: 
  - jhkim
date: 2023-10-02
meta_keywords: OpenSearch, Apache Kafka, Kafka Connect, Amazon MSK, Amazon MSK Connect, Docker
meta_description: Read on to learn about how to ingest data from Amazon MSK into OpenSaerch Service using MSK Connect on AWS.
twittercard:
  description: Learn about how to ingest data from Amazon MSK into OpenSaerch Service using MSK Connect on AWS.
category:
  - community
excerpt: 
  In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline will be deployed on AWS using Amazon MSK, Amazon MSK Connect and Amazon OpenSearch Service using Terraform in this post. First the infrastructure will be deployed that covers a VPC, VPN server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors will be deployed on MSK Connect, followed by performing quick data analysis.
feature_image: /assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/featured.png
---

In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline will be deployed on AWS using [Amazon MSK](https://aws.amazon.com/msk/), [Amazon MSK Connect](https://aws.amazon.com/msk/features/msk-connect/) and [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) using [Terraform](https://www.terraform.io/) in this post. First the infrastructure will be deployed that covers a Virtual Private Cloud (VPC), Virtual Private Network (VPN) server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors will be deployed on MSK Connect, followed by performing quick data analysis.

## Architecture

Fake impressions and clicks events are generated by the [Amazon MSK Data Generator](https://github.com/awslabs/amazon-msk-data-generator), and they are pushed into the corresponding Kafka topics. The topic records are ingested into OpenSearch indexes with the same names for near real-time analysis using the [Aiven's OpenSearch Connector for Apache Kafka](https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka). Note that, as the Kafka cluster and OpenSearch Service domain are deployed in private subnets, a VPN server is used to access them from the developer machine.

![architecture]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/featured.png){: .img-fluid }

## Infrastructure

The infrastructure is created using [Terraform](https://www.terraform.io/) and the source can be found in the [**GitHub repository**](https://github.com/jaehyeon-kim/general-demos/tree/master/opensearch-kafka-connect/remote) of this post.

### VPC and VPN

A VPC with 3 public and private subnets is created using the [AWS VPC Terraform module](https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest) (*vpc.tf*). Also, a [SoftEther VPN](https://www.softether.org/) server is deployed in order to access the resources in the private subnets from the developer machine (*vpn.tf*). The details about how to configure the VPN server can be found in [this post](https://jaehyeon.me/blog/2022-02-06-dev-infra-terraform/).

### OpenSearch Cluster

An OpenSearch domain that has two instances is created. It is deployed with the *m5.large.search* instance type in private subnets. For simplicity, [anonymous authentication](https://opensearch.org/docs/latest/security/configuration/configuration/#http) is enabled so that we don't have to specify user credentials when making an HTTP request. Overall only network-level security is enforced on the OpenSearch domain.

```terraform
# variable.tf
locals {
  ...
  opensearch = {
    engine_version = "2.7"
    instance_type  = "m5.large.search"
    instance_count = 2
  }
  ...
}

# opensearch.tf
resource "aws_opensearch_domain" "opensearch" {
  domain_name    = local.name
  engine_version = "OpenSearch_${local.opensearch.engine_version}"

  cluster_config {
    dedicated_master_enabled = false
    instance_type            = local.opensearch.instance_type  # m5.large.search
    instance_count           = local.opensearch.instance_count # 2
    zone_awareness_enabled   = true
  }

  advanced_security_options {
    enabled                        = false
    anonymous_auth_enabled         = true
    internal_user_database_enabled = true
  }

  domain_endpoint_options {
    enforce_https           = true
    tls_security_policy     = "Policy-Min-TLS-1-2-2019-07"
    custom_endpoint_enabled = false
  }

  ebs_options {
    ebs_enabled = true
    volume_size = 10
  }

  log_publishing_options {
    cloudwatch_log_group_arn = aws_cloudwatch_log_group.opensearch_log_group_index_slow_logs.arn
    log_type                 = "INDEX_SLOW_LOGS"
  }

  vpc_options {
    subnet_ids         = slice(module.vpc.private_subnets, 0, local.opensearch.instance_count)
    security_group_ids = [aws_security_group.opensearch.id]
  }

  access_policies = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action    = "es:*",
        Principal = "*",
        Effect    = "Allow",
        Resource  = "arn:aws:es:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:domain/${local.name}/*"
      }
    ]
  })
}
```

#### OpenSearch Security Group

The security group of the OpenSearch domain has inbound rules that allow connection from the security groups of the MSK cluster and VPN server. Only port 443 and 9200 are open for accessing the OpenSearch Dashboards and making HTTP requests.

```terraform
# opensearch.tf
resource "aws_security_group" "opensearch" {
  name   = "${local.name}-opensearch-sg"
  vpc_id = module.vpc.vpc_id

  lifecycle {
    create_before_destroy = true
  }

  tags = local.tags
}

resource "aws_security_group_rule" "opensearch_msk_inbound_https" {
  type                     = "ingress"
  description              = "Allow inbound traffic for OpenSearch Dashboard from MSK"
  security_group_id        = aws_security_group.opensearch.id
  protocol                 = "tcp"
  from_port                = 443
  to_port                  = 443
  source_security_group_id = aws_security_group.msk.id
}

resource "aws_security_group_rule" "opensearch_msk_inbound_rest" {
  type                     = "ingress"
  description              = "Allow inbound traffic for OpenSearch REST API from MSK"
  security_group_id        = aws_security_group.opensearch.id
  protocol                 = "tcp"
  from_port                = 9200
  to_port                  = 9200
  source_security_group_id = aws_security_group.msk.id
}

resource "aws_security_group_rule" "opensearch_vpn_inbound_https" {
  count                    = local.vpn.to_create ? 1 : 0
  type                     = "ingress"
  description              = "Allow inbound traffic for OpenSearch Dashboard from VPN"
  security_group_id        = aws_security_group.opensearch.id
  protocol                 = "tcp"
  from_port                = 443
  to_port                  = 443
  source_security_group_id = aws_security_group.vpn[0].id
}

resource "aws_security_group_rule" "opensearch_vpn_inbound_rest" {
  count                    = local.vpn.to_create ? 1 : 0
  type                     = "ingress"
  description              = "Allow inbound traffic for OpenSearch REST API from VPN"
  security_group_id        = aws_security_group.opensearch.id
  protocol                 = "tcp"
  from_port                = 9200
  to_port                  = 9200
  source_security_group_id = aws_security_group.vpn[0].id
}
```

### MSK Cluster

A MSK cluster with 2 brokers is created. The broker nodes are deployed with the *kafka.m5.large* instance type in private subnets and IAM authentication is used for the client authentication method. Finally, additional server configurations are added such as enabling auto creation of topics and topic deletion.

```terraform
# variable.tf
locals {
  ...
  msk = {
    version                    = "2.8.1"
    instance_size              = "kafka.m5.large"
    ebs_volume_size            = 20
    log_retention_ms           = 604800000 # 7 days
    number_of_broker_nodes     = 2
    num_partitions             = 2
    default_replication_factor = 2
  }
  ...
}

# msk.tf
resource "aws_msk_cluster" "msk_data_cluster" {
  cluster_name           = "${local.name}-msk-cluster"
  kafka_version          = local.msk.version
  number_of_broker_nodes = local.msk.number_of_broker_nodes
  configuration_info {
    arn      = aws_msk_configuration.msk_config.arn
    revision = aws_msk_configuration.msk_config.latest_revision
  }

  broker_node_group_info {
    instance_type   = local.msk.instance_size
    client_subnets  = slice(module.vpc.private_subnets, 0, local.msk.number_of_broker_nodes)
    security_groups = [aws_security_group.msk.id]
    storage_info {
      ebs_storage_info {
        volume_size = local.msk.ebs_volume_size
      }
    }
  }

  client_authentication {
    sasl {
      iam = true
    }
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_cluster_lg.name
      }
      s3 {
        enabled = true
        bucket  = aws_s3_bucket.default_bucket.id
        prefix  = "logs/msk/cluster/"
      }
    }
  }

  tags = local.tags

  depends_on = [aws_msk_configuration.msk_config]
}

resource "aws_msk_configuration" "msk_config" {
  name = "${local.name}-msk-configuration"

  kafka_versions = [local.msk.version]

  server_properties = <<PROPERTIES
    auto.create.topics.enable = true
    delete.topic.enable = true
    log.retention.ms = ${local.msk.log_retention_ms}
    num.partitions = ${local.msk.num_partitions}
    default.replication.factor = ${local.msk.default_replication_factor}
  PROPERTIES
}
```

#### MSK Security Group

The security group of the MSK cluster allows all inbound traffic from itself and all outbound traffic into all IP addresses. The Kafka connectors will use the same security group and the former is necessary. Both the rules are configured too generously, however, we can limit the protocol and port ranges in production. Also, the security group has an additional inbound rule that permits it to connect on port 9098 from the security group of the VPN server.

```terraform
resource "aws_security_group" "msk" {
  name   = "${local.name}-msk-sg"
  vpc_id = module.vpc.vpc_id

  lifecycle {
    create_before_destroy = true
  }

  tags = local.tags
}

resource "aws_security_group_rule" "msk_self_inbound_all" {
  type                     = "ingress"
  description              = "Allow ingress from itself - required for MSK Connect"
  security_group_id        = aws_security_group.msk.id
  protocol                 = "-1"
  from_port                = "0"
  to_port                  = "0"
  source_security_group_id = aws_security_group.msk.id
}

resource "aws_security_group_rule" "msk_self_outbound_all" {
  type              = "egress"
  description       = "Allow outbound all"
  security_group_id = aws_security_group.msk.id
  protocol          = "-1"
  from_port         = "0"
  to_port           = "0"
  cidr_blocks       = ["0.0.0.0/0"]
}

resource "aws_security_group_rule" "msk_vpn_inbound" {
  count                    = local.vpn.to_create ? 1 : 0
  type                     = "ingress"
  description              = "Allow VPN access"
  security_group_id        = aws_security_group.msk.id
  protocol                 = "tcp"
  from_port                = 9098
  to_port                  = 9098
  source_security_group_id = aws_security_group.vpn[0].id
}
```

### Deploy Infrastructure

To separate infrastructure deployment from Kafka connector creation, I added a Terraform variable called *to_create_connector*. As can be seen below, if we set the value to *false*, the Terraform resources will be created without MSK connectors. Below shows how to deploy the infrastructure.

```bash
$ terraform init
$ terraform plan -var to_create_connector=false
$ terraform apply --auto-approve=true -var to_create_connector=false
```

Once completed, we can check the two key resources on AWS Console - OpenSearch domain and MSK cluster.

![opensearch]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/aws-console-opensearch.png.png){: .img-fluid }

![msk]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/aws-console-msk-cluster.png){: .img-fluid }

#### Kafka Management App

A Kafka management app can be a good companion for development as it helps monitor and manage resources on an easy-to-use user interface. We'll use [Kpow Community Edition](https://docs.kpow.io/ce/) in this post, and we can link a single Kafka cluster, Kafka connect server and schema registry. Note that the community edition is valid for 12 months and the license can be requested in this [page](https://kpow.io/get-started/#individual). Once requested, the license details will be emailed, and they can be added as an environment file (*env_file*).

The app needs additional configurations in environment variables because the Kafka cluster on Amazon MSK is [authenticated by IAM](https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html). The bootstrap server address can be found on AWS Console or executing the following Terraform command. 

```bash
$ terraform output -json | jq -r '.msk_bootstrap_brokers_sasl_iam.value'
```

It also requires AWS credentials and SASL security configurations for IAM Authentication. Finally, I added an environment variable that indicates in which AWS region the MSK connectors are deployed. For further details, see the [Kpow documentation](https://docs.kpow.io/config/msk/).

```yaml
# docker-compose.yml
version: "3"

services:
  kpow:
    image: factorhouse/kpow-ce:91.2.1
    container_name: kpow
    ports:
      - "3000:3000"
    networks:
      - appnet
    environment:
      AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY
      AWS_SESSION_TOKEN: $AWS_SESSION_TOKEN
      # MSK cluster
      BOOTSTRAP: $BOOTSTRAP_SERVERS
      SECURITY_PROTOCOL: SASL_SSL
      SASL_MECHANISM: AWS_MSK_IAM
      SASL_CLIENT_CALLBACK_HANDLER_CLASS: software.amazon.msk.auth.iam.IAMClientCallbackHandler
      SASL_JAAS_CONFIG: software.amazon.msk.auth.iam.IAMLoginModule required;
      # MSK connect
      CONNECT_AWS_REGION: $AWS_DEFAULT_REGION

networks:
  appnet:
    name: app-network
```

## Data Pipeline

### Preparation
#### Download Connector Sources

The connector sources need to be downloaded into the `./connectors` path so that they can be volume-mapped to the container's plugin path (`/opt/connectors`). The MSK Data Generator is a single Jar file, and it can be kept as it is. On the other hand, the Aiven OpenSearch sink connector is an archive file, and it should be decompressed. Note the zip file will be used to create a custom plugin of MSK Connect in the next post. The following script downloads the connector sources into the host path.

```bash
# download.sh
#!/usr/bin/env bash
shopt -s extglob

SCRIPT_DIR="$(cd $(dirname "$0"); pwd)"

SRC_PATH=${SCRIPT_DIR}/connectors
rm -rf ${SRC_PATH} && mkdir ${SRC_PATH}

## Avien opensearch sink connector
echo "downloading opensearch sink connector..."
DOWNLOAD_URL=https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka/releases/download/v3.1.0/opensearch-connector-for-apache-kafka-3.1.0.zip

curl -L -o ${SRC_PATH}/tmp.zip ${DOWNLOAD_URL} \
  && unzip -qq ${SRC_PATH}/tmp.zip -d ${SRC_PATH} \
  && rm -rf $SRC_PATH/!(opensearch-connector-for-apache-kafka-3.1.0) \
  && mv $SRC_PATH/opensearch-connector-for-apache-kafka-3.1.0 $SRC_PATH/opensearch-connector \
  && cd $SRC_PATH/opensearch-connector \
  && zip ../opensearch-connector.zip *

## MSK Data Generator Souce Connector
echo "downloading msk data generator..."
DOWNLOAD_URL=https://github.com/awslabs/amazon-msk-data-generator/releases/download/v0.4.0/msk-data-generator-0.4-jar-with-dependencies.jar

mkdir ${SRC_PATH}/msk-datagen \
  && curl -L -o ${SRC_PATH}/msk-datagen/msk-data-generator.jar ${DOWNLOAD_URL}
```

Below shows the folder structure after the connectors are downloaded successfully.

```bash
$ tree connectors/
connectors/
├── msk-datagen
│   └── msk-data-generator.jar
├── opensearch-connector

...

│   ├── opensearch-2.6.0.jar
│   ├── opensearch-cli-2.6.0.jar
│   ├── opensearch-common-2.6.0.jar
│   ├── opensearch-connector-for-apache-kafka-3.1.0.jar
│   ├── opensearch-core-2.6.0.jar

...

└── opensearch-connector.zip

2 directories, 56 files
```

#### Create Index Mappings

The topic messages include a timestamp field (*created_at*), but its type is not identified correctly via dynamic mapping. Instead, indexes are created explicitly as shown below. Note that we don't have to specify user credentials because anonymous authentication is enabled. Make sure to connect to the VPN server before executing this script.

```bash
# configs/create-index-mappings.sh
#!/usr/bin/env bash
OPENSEARCH_ENDPOINT=$(terraform output -json | jq -r '.opensearch_domain_endpoint.value')
echo "OpenSearch endpoint - $OPENSEARCH_ENDPOINT ..."

echo "Create impressions index and field mapping"
curl -X PUT "https://$OPENSEARCH_ENDPOINT/impressions" -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "bid_id": {
        "type": "text"
      },
      "created_at": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss"
      },
      "campaign_id": {
        "type": "text"
      },
      "creative_details": {
        "type": "keyword"
      },
      "country_code": {
        "type": "keyword"
      }
    }
  }
}'

echo
echo "Create clicks index and field mapping"
curl -X PUT "https://$OPENSEARCH_ENDPOINT/clicks" -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "correlation_id": {
        "type": "text"
      },
      "created_at": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss"
      },
      "tracker": {
        "type": "text"
      }
    }
  }
}'

echo
```

Once created, we can check them in the OpenSearch Dashboards where its endpoint can be found by executing the following Terraform command. 

```bash
$ terraform output -json | jq -r '.opensearch_domain_dashboard_endpoint.value'
```

![index_creation]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/index-creation.png){: .img-fluid }

#### Connector IAM Role

For simplicity, a single IAM role will be used for both the source and sink connectors. The custom managed policy has permission on MSK cluster resources (cluster, topic and group). It also has permission on S3 bucket and CloudWatch Log for logging.

```terraform
# msk-connect.tf
resource "aws_iam_role" "kafka_connector_role" {
  name = "${local.name}-connector-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Sid    = ""
        Principal = {
          Service = "kafkaconnect.amazonaws.com"
        }
      },
    ]
  })
  managed_policy_arns = [
    "arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess",
    aws_iam_policy.kafka_connector_policy.arn
  ]
}

resource "aws_iam_policy" "kafka_connector_policy" {
  name = "${local.name}-connector-policy"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid = "PermissionOnCluster"
        Action = [
          "kafka-cluster:Connect",
          "kafka-cluster:AlterCluster",
          "kafka-cluster:DescribeCluster"
        ]
        Effect   = "Allow"
        Resource = "arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:cluster/${local.name}-msk-cluster/*"
      },
      {
        Sid = "PermissionOnTopics"
        Action = [
          "kafka-cluster:*Topic*",
          "kafka-cluster:WriteData",
          "kafka-cluster:ReadData"
        ]
        Effect   = "Allow"
        Resource = "arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:topic/${local.name}-msk-cluster/*"
      },
      {
        Sid = "PermissionOnGroups"
        Action = [
          "kafka-cluster:AlterGroup",
          "kafka-cluster:DescribeGroup"
        ]
        Effect   = "Allow"
        Resource = "arn:aws:kafka:${local.region}:${data.aws_caller_identity.current.account_id}:group/${local.name}-msk-cluster/*"
      },
      {
        Sid = "PermissionOnDataBucket"
        Action = [
          "s3:ListBucket",
          "s3:*Object"
        ]
        Effect = "Allow"
        Resource = [
          "${aws_s3_bucket.default_bucket.arn}",
          "${aws_s3_bucket.default_bucket.arn}/*"
        ]
      },
      {
        Sid = "LoggingPermission"
        Action = [
          "logs:CreateLogStream",
          "logs:CreateLogGroup",
          "logs:PutLogEvents"
        ]
        Effect   = "Allow"
        Resource = "*"
      },
    ]
  })
}
```

### Source Connector

The connector source will be uploaded into S3 followed by creating a customer plugin. Then the source connector will be created using the custom plugin and deployed in private subnets. 

When it comes to the connector configuration, the first six attributes are in relation to general configurations. The connector class (*connector.class*) is required for any connector and I set it for the MSK Data Generator. Also, two tasks are allocated to it (*tasks.max*). The message key is set to be converted into string (*key.converter*) while the value to json (*value.converter*). The former is because the keys are configured to have string primitive values (*genkp*) by the source connector. Finally, schemas are not enabled for both the key and value.

The remaining attributes are for the MSK Data Generator. Two topics named *impressions* and *clicks* will be created, and the messages attributes are generated by the [Java faker library](https://github.com/DiUS/java-faker). Interestingly the bid ID of the impression message and the correlation ID of the click message share the same value *sometimes*. This is because only a fraction of impressions results in clicks in practice.

```terraform
resource "aws_mskconnect_connector" "msk_data_generator" {
  count = var.to_create_connector ? 1 : 0
  name  = "${local.name}-ad-tech-source"

  kafkaconnect_version = "2.7.1"

  capacity {
    provisioned_capacity {
      mcu_count    = 1
      worker_count = 1
    }
  }

  connector_configuration = {
    # connector configuration
    "connector.class"                = "com.amazonaws.mskdatagen.GeneratorSourceConnector",
    "tasks.max"                      = "2",
    "key.converter"                  = "org.apache.kafka.connect.storage.StringConverter",
    "key.converter.schemas.enable"   = false,
    "value.converter"                = "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable" = false,
    # msk data generator configuration
    "genkp.impressions.with"                        = "#{Code.isbn10}"
    "genv.impressions.bid_id.with"                  = "#{Code.isbn10}"
    "genv.impressions.campaign_id.with"             = "#{Code.isbn10}"
    "genv.impressions.creative_details.with"        = "#{Color.name}"
    "genv.impressions.country_code.with"            = "#{Address.countryCode}"
    "genkp.clicks.with"                             = "#{Code.isbn10}"
    "genv.clicks.correlation_id.sometimes.matching" = "impressions.value.bid_id"
    "genv.clicks.correlation_id.sometimes.with"     = "NA"
    "genv.clicks.tracker.with"                      = "#{Lorem.characters '15'}"
    "global.throttle.ms"                            = "500"
    "global.history.records.max"                    = "1000"
  }

  kafka_cluster {
    apache_kafka_cluster {
      bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam

      vpc {
        security_groups = [aws_security_group.msk.id]
        subnets         = module.vpc.private_subnets
      }
    }
  }

  kafka_cluster_client_authentication {
    authentication_type = "IAM"
  }

  kafka_cluster_encryption_in_transit {
    encryption_type = "TLS"
  }

  plugin {
    custom_plugin {
      arn      = aws_mskconnect_custom_plugin.msk_data_generator.arn
      revision = aws_mskconnect_custom_plugin.msk_data_generator.latest_revision
    }
  }

  log_delivery {
    worker_log_delivery {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_data_generator.name
      }
      s3 {
        enabled = true
        bucket  = aws_s3_bucket.default_bucket.id
        prefix  = "logs/msk/connect/msk-data-generator"
      }
    }
  }

  service_execution_role_arn = aws_iam_role.kafka_connector_role.arn
}

resource "aws_mskconnect_custom_plugin" "msk_data_generator" {
  name         = "${local.name}-msk-data-generator"
  content_type = "JAR"

  location {
    s3 {
      bucket_arn = aws_s3_bucket.default_bucket.arn
      file_key   = aws_s3_object.msk_data_generator.key
    }
  }
}

resource "aws_s3_object" "msk_data_generator" {
  bucket = aws_s3_bucket.default_bucket.id
  key    = "plugins/msk-data-generator.jar"
  source = "connectors/msk-datagen/msk-data-generator.jar"

  etag = filemd5("connectors/msk-datagen/msk-data-generator.jar")
}

resource "aws_cloudwatch_log_group" "msk_data_generator" {
  name = "/msk/connect/msk-data-generator"

  retention_in_days = 1

  tags = local.tags
}
```

### Sink Connector

Similar to the source connector, the sink connector will be deployed using a customer plugin where its source is uploaded into S3. It is marked to depend on the source connector so that it will be created only after the source connector is deployed successfully. 

For connector configuration, it is configured to write messages from the *impressions* and *clicks* topics into the OpenSearch indexes created earlier. It uses the same key and value converters to the source connector, and schemas are not enabled for both the key and value.

The OpenSearch domain endpoint is added to the connection URL attribute (*connection.url*), and this is the only necessary attribute for making HTTP requests thanks to anonymous authentication. Also, as the topics are *append-only* logs, we can set the document ID to be *[topic-name].[partition].[offset]* by setting *key.ignore* to *true*. See the [connector configuration document](https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka/blob/main/docs/opensearch-sink-connector-config-options.rst) for more details. 

Having an event timestamp attribute can be useful for performing temporal analysis. As I don't find a comprehensive way to set it up in the source connector, a new field called *created_at* is added using [single message transforms (SMTs)](https://kafka.apache.org/documentation.html#connect_transforms). Specifically I added two transforms - *insertTS* and *formatTS*. As the name suggests, the former inserts the system timestamp value while it is formatted into *yyyy-MM-dd HH:mm:ss* by the latter.

```terraform
resource "aws_mskconnect_connector" "opensearch_sink" {
  count = var.to_create_connector ? 1 : 0
  name  = "${local.name}-ad-tech-sink"

  kafkaconnect_version = "2.7.1"

  capacity {
    provisioned_capacity {
      mcu_count    = 1
      worker_count = 1
    }
  }

  connector_configuration = {
    # connector configuration
    "connector.class"                = "io.aiven.kafka.connect.opensearch.OpensearchSinkConnector",
    "tasks.max"                      = "2",
    "topics"                         = "impressions,clicks",
    "key.converter"                  = "org.apache.kafka.connect.storage.StringConverter",
    "key.converter.schemas.enable"   = false,
    "value.converter"                = "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable" = false,
    # opensearch sink configuration
    "connection.url"                  = "https://${aws_opensearch_domain.opensearch.endpoint}",
    "schema.ignore"                   = true,
    "key.ignore"                      = true,
    "type.name"                       = "_doc",
    "behavior.on.malformed.documents" = "fail",
    "behavior.on.null.values"         = "ignore",
    "behavior.on.version.conflict"    = "ignore",
    # dead-letter-queue configuration
    "errors.deadletterqueue.topic.name"               = "ad-tech-dl",
    "errors.tolerance"                                = "all",
    "errors.deadletterqueue.context.headers.enable"   = true,
    "errors.deadletterqueue.topic.replication.factor" = "1",
    # single message transforms
    "transforms"                          = "insertTS,formatTS",
    "transforms.insertTS.type"            = "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.insertTS.timestamp.field" = "created_at",
    "transforms.formatTS.type"            = "org.apache.kafka.connect.transforms.TimestampConverter$Value",
    "transforms.formatTS.format"          = "yyyy-MM-dd HH:mm:ss",
    "transforms.formatTS.field"           = "created_at",
    "transforms.formatTS.target.type"     = "string"
  }

  kafka_cluster {
    apache_kafka_cluster {
      bootstrap_servers = aws_msk_cluster.msk_data_cluster.bootstrap_brokers_sasl_iam

      vpc {
        security_groups = [aws_security_group.msk.id]
        subnets         = module.vpc.private_subnets
      }
    }
  }

  kafka_cluster_client_authentication {
    authentication_type = "IAM"
  }

  kafka_cluster_encryption_in_transit {
    encryption_type = "TLS"
  }

  plugin {
    custom_plugin {
      arn      = aws_mskconnect_custom_plugin.opensearch_sink.arn
      revision = aws_mskconnect_custom_plugin.opensearch_sink.latest_revision
    }
  }

  log_delivery {
    worker_log_delivery {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.opensearch_sink.name
      }
      s3 {
        enabled = true
        bucket  = aws_s3_bucket.default_bucket.id
        prefix  = "logs/msk/connect/opensearch-sink"
      }
    }
  }

  service_execution_role_arn = aws_iam_role.kafka_connector_role.arn

  depends_on = [
    aws_mskconnect_connector.msk_data_generator
  ]
}

resource "aws_mskconnect_custom_plugin" "opensearch_sink" {
  name         = "${local.name}-opensearch-sink"
  content_type = "ZIP"

  location {
    s3 {
      bucket_arn = aws_s3_bucket.default_bucket.arn
      file_key   = aws_s3_object.opensearch_sink.key
    }
  }
}

resource "aws_s3_object" "opensearch_sink" {
  bucket = aws_s3_bucket.default_bucket.id
  key    = "plugins/opensearch-connector.zip"
  source = "connectors/opensearch-connector.zip"

  etag = filemd5("connectors/opensearch-connector.zip")
}

resource "aws_cloudwatch_log_group" "opensearch_sink" {
  name = "/msk/connect/opensearch-sink"

  retention_in_days = 1

  tags = local.tags
}
```

### Deploy Connectors

The connectors can be deployed while setting the value of *to_create_connector* to *true* as shown below.

```bash
$ terraform plan -var to_create_connector=true
$ terraform apply --auto-approve=true -var to_create_connector=true
```

Once completed, we can check the source and sink connectors on AWS Console as following.

![source_connector]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/source-connector.png){: .img-fluid }

![sink_connector]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/sink-connector.png){: .img-fluid }

#### Source Data

We can use Kpow to see the details of the impressions and clicks topics on *localhost:3000*. Make sure to connect to the VPN server, or it fails to access the MSK cluster.

![kafka_topics]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/kafka-topics.png){: .img-fluid }

As mentioned earlier, only a fraction of correlation IDs of the click messages has actual values, and we can see that by inspecting the messages of the clicks topic.

![click_messages]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/click-messages.png){: .img-fluid }

#### OpenSearch Dashboard

In OpenSearch Dashboards, we can search clicks that are associated with impressions. As expected, only a small portion of clicks are searched. 

![search_result]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/result-query.png){: .img-fluid }

Moreover, we can join correlated impressions and clicks quickly using the [Query Workbench](https://opensearch.org/docs/latest/search-plugins/sql/sql/index/). Below shows a simple SQL query that joins impressions and associating clicks that are created after a certain time point.

![query_join]({{ site.baseurl }}/assets/media/blog-images/2023-10-02-Ingest-data-from-Kafka-into-OpenSearch-using-Kafka-Connect-Part-2-AWS-Deployment/result-join.png){: .img-fluid }

## Destroy Resources

As all resources are created by Terraform, they can be destroyed by a single command as shown below.

```bash
$ terraform destroy --auto-approve=true -var to_create_connector=true
```

## Summary

In the previous post, we discussed how to develop a data pipeline from Apache Kafka into OpenSearch locally using Docker. The pipeline was deployed on AWS using Amazon MSK, Amazon MSK Connect and Amazon OpenSearch Service using Terraform in this post. First the infrastructure was deployed that covers a Virtual Private Cloud (VPC), Virtual Private Network (VPN) server, MSK Cluster and OpenSearch domain. Then Kafka source and sink connectors were deployed on MSK Connect, followed by performing quick data analysis. It turns out that Kafka Connect can be effective for ingesting data from Kafka into OpenSearch.