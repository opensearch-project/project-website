---
layout: post
title:  "The ABCs of semantic search in OpenSearch: Architectures, benchmarks, and combination strategies"
authors:
 - kschnitter
date: 2024-05-03
categories:
 - technical-post
meta_keywords: 
meta_description: 
excerpt: OpenSearch provides OpenTelemetry ingestion by using Data Prepper as an ingestion tool. In this blog post, we explore how to analyze and visualize metrics indexed using this method.
has_math: true
has_science_table: true
---

OpenSearch is growing into a full observability platform with support for logs, metrics, and traces.
Using [Data Prepper](https://opensearch.org/docs/latest/data-prepper/), you can ingest these three signal types in OpenTelemetry (OTel) format.
_Logs_ are well supported by the search index and other core OpenSearch and OpenSearch Dashboards features.
_Traces_ have their own section within the Observability plugin, providing flame graphs, service maps, and much more.
OpenSearch _metrics_ support is geared toward the integration of an external Prometheus data source.
Analysis of metrics ingested using Data Prepper requires custom-built visualizations.
This blog post explores these visualizations and the underlying data model.
It is built on [SAP Cloud Logging](https://discovery-center.cloud.sap/serviceCatalog/cloud-logging), through which I created predefined dashboards for customers.

### Outline

- [Example setup](#example-setup)
- [OpenTelemetry data model](#opentelemetry-metrics-data-model)
- [Discover and search the data](#discover-inspecting-the-raw-data-and-saved-searches)
- [Visualizations with TSVB](#visualizations-with-tsvb)
- [Standard visualizations](#standard-visualizations)
- [Vega](#outlook-vega)

### Example setup

Let's start with a common and easily reproducible metrics generation scenario.
In this case, we can assume a running Kubernetes (K8s) cluster for which we want to analyze container metrics.
These metrics can be generated by the [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) using the [Kubelet Stats Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md).
See the linked documentation for information about how to best set up the receiver in your cluster.
There are several options for authentication to the K8s endpoints.

To export data to Data Prepper, all you need is an `otlp` exporter using the gRPC protocol:

```yaml
exporters:
  otlp:
    endpoint: "<your-DataPrepper-url>"
    tls:
      cert_file: "./cert.pem"
      key_file: "./key.pem"
```

This example uses mTLS, but you can also use other authentication options.
The corresponding Data Prepper configuration looks something like this:

```yaml
metrics-pipeline:
    source:
    otel_metrics_source:
        ssl: true
        sslKeyCertChainFile: "./cert.pem"
        sslKeyFile: "./key.pem"
    processor:
    - otel_metrics:
    sink:
    - opensearch:
        hosts: [ "<your-OpenSearch-url>" ]
        username: "otelWriter"
        password: "<your-password>"
        index: metrics-otel-v1-%{yyyy.MM.dd}
```

This example uses basic authentication for OpenSearch.
It requires the `otelWriter` backend role with sufficient permissions for Data Prepper.
This is similar to trace ingestion.

The complete data flow of this setup is shown in the following diagram.

![Data Flow of OTel Ingestion](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/otel-ingestion-architecture.drawio.png){:class="img-centered"}

When this setup is complete, you will have a lot of metrics in OpenSearch to analyze.
See the [kubeletstats documentation](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/documentation.md) for information about the metrics and metadata emitted by the Kubelet Stats Receiver.

> Hint: If you are a service provider for development teams, this setup can be automated and transparent to your users.
Within SAP BTP, this is achieved by the [Kyma Telemetry module](https://kyma-project.io/#/telemetry-manager/user/README) in connection with [SAP Cloud Logging](https://discovery-center.cloud.sap/serviceCatalog/cloud-logging).
It is this solution that forms the foundation of this blog post.

### OpenTelemetry metrics data model

OTel uses a well-documented [Metrics Data Model](https://opentelemetry.io/docs/specs/otel/metrics/data-model/).
The linked article provides information about the various supported metric types.
We will take a less thorough approach and inspect example points to collect the necessary information.
With our example setup, we can find data points similar to this (shortened) example:

```json
{
  "_source": {
    "name": "container.cpu.time",
    "description": "Total cumulative CPU time (sum of all cores) spent by the container/pod/node since its creation",
    "value": 120.353905,
    "flags": 0,
    "unit": "s",
    "startTime": "2024-04-10T06:49:56Z",
    "time": "2024-05-03T07:50:22.197391185Z",
    "kind": "SUM",
    "aggregationTemporality": "AGGREGATION_TEMPORALITY_CUMULATIVE",
    "isMonotonic": true,
    "serviceName": "otel-emailservice",
    "instrumentationScope.name": "otelcol/kubeletstatsreceiver",
    "instrumentationScope.version": "0.97.0",
    "resource.attributes.k8s@namespace@name": "example",
    "resource.attributes.k8s@deployment@name": "otel-emailservice",
    "resource.attributes.k8s@pod@name": "otel-emailservice-7795877686-r2lrt",
    "resource.attributes.k8s@container@name": "emailservice",
    "resource.attributes.service@name": "otel-emailservice"
  }
}
```

We can see that OTel separates metrics by the `name`.
Each point contains the generic `value` and `time`, which form the basic time series for the visualizations.
This schema is a at first glance suboptimal for OpenSearch, where having a field with key `name` and a value containing the numerical value is more easily accessible.
For our example, this would look like `{ "container.cpu.time": 120.353905}`.
However, if we ingest a lot of different metrics, we may reach the field limit of 1,000 per index fairly quickly.

There is additional information about the kind that introduces semantics to the time series.
This example uses a "SUM" (a counter), which is monotonic.
The "AGGREGATION_TEMPORALITY_CUMULATIVE" tells us that the `value` will contain the current count started at `startTime`.
The alternative "AGGREGATION_TEMPORALITY_DELTA" would contain only the difference between `startTime` and `time` with non-overlapping intervals.
We need to take these semantics into account, for example, if we want to visualize rate of change.

Finally, there are additional metadata fields that we can use to slice our data by different dimensions.
There are three different groups:

- _Resource attributes_ describe the metric source. You can see the K8s coordinates in the example.
- _Metric attributes_ describe particular aspects of the metric. There is no such attribute in our example, but you can think of memory pools as working similarly.
- _Instrumentation scope_ describes the method of metrics acquisition. In our case, this is the Kubelet Stats Receiver of the OpenTelemetry Collector.

Note that the attributes contain a lot of "@" characters.
In the [OpenTelemetry semantic conventions](https://github.com/open-telemetry/semantic-conventions) that specify attributes and metric names, there are dots "." instead.
Because OpenSearch treats those dots as nested fields, Data Prepper applies this . -> @ mapping to avoid conflicts.

> Hint: If you want to manipulate OTel data with Piped Processing Language (PPL) in OpenSearch, you need to use backticks around field names containing the @ symbol.

Now we have everything we need to create our first visualizations: We need to access the `value` over `time` selected by `name`. Additional dimensions come from the attributes.
  
### Discover: Inspecting the raw data and saved searches

Now that we know what [data model](#opentelemetry-metrics-data-model) to expect, we can investigate the metrics using the Discover plugin.
The following screenshot shows metrics for the [OpenTelemetry Demo](https://github.com/open-telemetry/opentelemetry-demo) collected using our [example setup](#example-setup).

![Data Flow of OTel Ingestion](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/discover_otel-metrics_general.png){:class="img-centered"}

Note the filtering by instrumentation scope and K8s namespace.

We can use the Discover plugin to develop Dashboards Query Language (DQL) expressions for data filtering, such as `name:k8s.pod.cpu.time`, or create saved searches.
Both will be used for visualizations.

One of the main benefits of Discover is easy access to metric attributes.
The plugin provides discovery of present attributes and their potential values.

### Visualizations with TSVB

The Time-Series Visual Builder (TSVB) offers a class of visualizations specifically for time-series data, which will work very well for the metrics we want to analyze.
We start at the **Visualize plugin**.

![Visualize Overview](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_overview.png){:class="img-centered"}

We create a new visualization and choose **TSVB**.

![New TSVB Visualization](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_create_tsvb.png){:class="img-centered"}

As a first step, we navigate to **Panel options** below the graph.

![TSVB Panel Options](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_panel-options.png){:class="img-centered"}

We select:
- Our example index pattern `metrics-otel-v1-*`
- The time field `time` from the Data Prepper/OTel data model
- An interval of at least 1 minute
- The panel filter DQL expression `name:k8s.pod.cpu.time` we explored with [Discover](#discover-inspecting-the-raw-data-and-saved-searches)

The index pattern might be different in your configuration.
Use the same pattern as in [Discover](#discover-inspecting-the-raw-data-and-saved-searches).

The minimum interval is important for shorter search intervals.
TSVB will calculate the interval length automatically and only connect points between adjacent intervals.
If there are intervals without values, the dots will not be connected.

Choosing the panel filter `name:k8s.pod.cpu.time` filters the metrics for the Pod CPU time.
This is the metric we want to visualize first.

We now switch back to the **Data** tab.

![TSVB Data Plain](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-max-value.png){:class="img-centered"}

Here we will configure the data that we want to visualize.
We start by selecting aggregation "Max" of field "value".
This aligns well with the cumulative aggregation temporality.
In each interval the maximum (last) CPU time is placed as a dot on the diagram.
We group by a "Terms" aggregation on the K8s pod name to get the CPU times per pod.
To avoid overcrowding of our graph, we select **Top 3** ordered by "Max of value" in descending direction.
This provides us with a timeline of CPU time by the top 3 pods.

If we were visualizing a metric of kind gauge, this would be it.
However, with our sum, there is more to be done.
The graph looks fine, but the arbitrary `startTime` differing between the pods makes the values hard to compare.
In the next step we calculate the rate of our sum.

![TSVB Data Positive Rate](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-positive-rate.png){:class="img-centered"}

We extend the aggregations by a derivative by unit 1s.
This calculates the time share spent by the CPU on that pod.
We add another aggregation to filter for positive values only.
This suppresses artifacts from pod restarts.

Note that as a shortcut, you can replace the original "Max" aggregation with "Positive Rate".
However, this comes at a cost to the ordering.
On the bottom line of the screenshot, you can see that we can only sort by the "max of value".
This is the best we can do.
If we used "Positive Rate", even that option would vanish.

We can improve the graph by choosing percent as Data Formatter under the Options tab.
This provides a nice view of the CPU utilization history by pod. 

![TSVB Data Options](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-options.png){:class="img-centered"}

We can re-aggregate the data of this timeline by a cumulative sum.
This yields the CPU time spent in the selected interval.
Let's also switch the graph type to "Top N".

![TSVB Data Cumulative](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-cumulative.png){:class="img-centered"}

Now we have an ordered bar chart of our top 10 pods (we need to adjust the number of options in the group by).

We have now created two different visualizations of pod CPU utilization.
We can save our visualizations at any point in time and add them to dashboards, that combine multiple visualizations.
TSVB allows you to configure whether global filters should be respected or not.
This allows you to embed the visualizations into larger scenarios sharing filters.
There are many more features in TSVB that allow for the creation of more elaborate graphs.
Highlights of TSVB are its relatively easy access to time-series visualization and its graph adjustment options, especially the option to adjust the data formatter.

### Standard visualizations

We will now take a look at the standard visualizations offered by OpenSearch Dashboards.
Compared to TSVB, these visualizations are a little more elaborate to build.
However, they allow the creation of filters that can be applied to the entire dashboard instead of only to the current visualization.
This makes them ideal as selectors.

#### A data table for K8s namespaces

We start with a data table showing us the number of pods and containers by namespace.
For this, we create a new visualization and choose **Data Table**.

![New Data Table Visualization](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_create_data-table.png){:class="img-centered"}

We are asked to select the index or saved search as data source.
For our table, we can use the entire OTel metrics index.
Let's take a look at the finished table and then walk through the steps required to generate it.

![K8s Pods and Containers per Namespace](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_unique-pod-names.png){:class="img-centered"}

Each table line lists the number of pods and containers.
Note that in the example, the "kyma-system" namespace consists of 23 pods all running just one Istio container.
This explains the lower number of unique containers compared to the number of pods.

On the right, we can see how the number of pods is calculated: It is the unique count of values in field `resource.attributes.k8s@pod@name`, which contains the pod name.
We could have used the pod ID as well, but pod names contain a unique suffix, so this is semantically identical.
Note that the pod name is filled in by the Kubelet Stats Receiver as resource attribute.

The container count can be added by selecting the **plus sign button** with a very similar configuration.

![K8s Unique Container Count](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_unique-container-names.png){:class="img-centered"}

Note that providing custom labels produces nicer table headings.
Any changes to our configuration are not applied automatically.
Use the **Update** button on the bottom right to sync the visualization.

We still need to provide separate lines per namespace.
This is achieved with a bucket aggregation.

![K8s Namespace Terms Aggregation](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_namespace-grouping.png){:class="img-centered"}

We choose "split rows" and a "Terms" aggregation for field `resource.attributes.k8s@namespace@name` and order our table by the number of pods.
We could also choose the number of containers or any of the other metrics we defined above, and there is also the option to use a completely separate metric.

We can also select the number of entries that we want to retrieve maximally.
The **Options** tab allows us to paginate this result by specifying the number of rows per page, and we can also select a "total" function on the same tab.
But in our case, only summing the number of pods will provide a correct number.
The container count might double count containers that appear in different namespaces, so this option was left unchecked.

On the **Data** tab, we could also opt to create a "missing values" bucket.
This may somewhat reduce query performance.
We can always check the query performance with the inspect dialog by looking at the **Requests** and **Response** tabs.

![Visualization Query Performance](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_performance.png){:class="img-centered"}

The value reported as `"took": 1787` indicates a latency of 1787 ms.
We can always use this feature to analyze the impact of different aggregations and metric configurations on the query runtime.
This can be very helpful for complicated visualizations.

One of the main benefits of standard visualizations is that they allow you to create global filters.
In our table, the first column allows filtering on a specific namespace (plus sign) or excluding it (minus sign).

![Visualization Filter Creation](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_selector.png){:class="img-centered"}

Selecting either icon will create the corresponding filter and reload the entire dashboard.
This makes the data tables good candidates for providing selectors by different dimensions.
The metrics can help users make decisions about what data to focus on.
In our example, we provide a general overview of the size and variety of a namespace.

#### A line chart for timelines

We will now push the standard line chart to its limits by trying to recreate the earlier TSVB visualization of the Pod CPU time.
Actually, we will go a little beyond that and attempt to visualize the container CPU time for each pod.

For our [data table](#a-data-table-for-k8s-namespaces) we created in the last subsection, we used an index pattern.
This time, we will create a proper saved search in the Discovery plugin.
We need to filter our OTel metrics index by the metric name `container.cpu.time`.

![Saved Search for container CPU time](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/discover_k8s-container-cpu-time.png){:class="img-centered"}

This screenshot shows both possibilities: Either use DQL in the search input or create the equivalent filter.
Our example contains an additional filter for the namespace to show the possibility of adding additional filters.
We save our search, so that we can reference it, when we create our line chart.
The major benefit of this approach is that we can refine our saved search during the creation of our visualization.

![New Line Chart Visualization](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_create_line-chart.png){:class="img-centered"}

![Select Saved Search](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_select-saved-search.png){:class="img-centered"}

We now need to provide several configurations.
Let's start with the x-axis, which needs to be a date histogram.
This divides the time interval selected on the upper right of the screen into time buckets.

![Date Histogram](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_date-histogram.png){:class="img-centered"}

The default time field for OTel metrics is `time`.
The minimum bucket length needs to be at least the reporting interval in order to avoid empty buckets.
The actual length will be selected from the chosen interval.

We can now configure our y-axis.

![Calculating the CPU Time Slice](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_y-axis.png){:class="img-centered"}

We want to use the derivative of each interval.
Because this is a parent aggregation, we need a time bucket aggregation as well.
Here we use the maximum because the metric is a monotonically increasing counter.

In comparison with TSVB, we have no control over the derivation scale.
This means that the derivative will be dependent on the interval length.
This is why we did not give the x-axis a custom label---so that it will print the interval length.
This is the first limitation of the standard line chart that we encounter.

Note that configuring the date histogram on the x-axis is required for using the derivative aggregation.

As a finishing touch, we will now split our data series by K8s pod name and K8s container name.
This is something we could not do in TSVB, which only supported grouping by one field.
We start with the pod name.

![Split by Pod Name](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_split_pod-name.png){:class="img-centered"}

Note that for ordering, we are again restricted to the max aggregation.
We cannot sort by the derivative well, just as with TSVB.
After the pod, we split again by container name.

![Split by Container Name](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_split_container-name.png){:class="img-centered"}

When you add the "split series" configuration, OpenSearch Dashboards notify you that the date histogram needs to be the last visualization.
You can rearrange the bucket aggregations by dragging and dropping them on the "=" icon.

With all that configuration done, we select the **Update** button to inspect the graph.

![CPU Time by Container and Pod](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_chart.png){:class="img-centered"}

We get a line of the CPU time used by time interval for each container.
The labels consist of the pod name first and the container name second.
We can control this ordering by changing the order of our aggregations.
Similarly to TSVB, we have a problem distinguishing long labels in the legend.
This is no better, if we bring the legend to the bottom with the panel options.

If we hover over a data point, we get additional information.

![Line Chart Point Info](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_data-point_info.png){:class="img-centered"}

We can see the timestamp and CPU time used.
The labels for pod and container names are presented but are hard to recognize.
This is caused by the double aggregation.
If we used just pod names, this would be much better.
An alternative approach would be to use "split series" for container names and "split chart" for pod names.

Selecting a data point brings up a filter creation dialog.

![Line Chart Point Filter](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_data-point_filters.png){:class="img-centered"}

We can use these options to slice our data.
These filters are applied across the entire dashboard.

We have pushed our timeline visualization to the limits of the standard charts.
This gives us a good understanding of its capabilities.
The visualization has its benefits in terms of creating filters and grouping by multiple attributes, but TSVB offers easier options for formatting data.

### Outlook: Vega

We have explored how to set up and present OTel metrics using TSVB and standard visualizations.
Both approaches allow the creation of useful charts that provide powerful insights, but they also present some limitations.
To overcome those limitations, OpenSearch Dashboards offers Vega as another powerful visualization tool.
Vega lets you use any OpenSearch query and provides a grammar for transforming and visualizing the data with [D3.js](https://d3js.org/).
It allows you to access global filters and the time interval selector to create well-integrated exploration and analysis journeys in OpenSearch.

Of course, this has a steeper learning curve.
Explaining Vega visualizations would warrant its own blog post.
See [Vega Visualizations](https://github.com/opensearch-project/opensearch-catalog/blob/main/visualizations/vega-visualizations.md) for a short introduction and to get further inspiration.